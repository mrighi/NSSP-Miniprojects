{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python39\\lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "c:\\Python39\\lib\\site-packages\\numpy\\.libs\\libopenblas.FB5AE2TYXYH2IJRDKGDGQ3XBKLKTF43H.gfortran-win_amd64.dll\n",
      "c:\\Python39\\lib\\site-packages\\numpy\\.libs\\libopenblas.XWYDX2IKJW2NMTWSFYNGFUWKQU3LYTCZ.gfortran-win_amd64.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n",
      "c:\\Python39\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# Section 1\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "import copy\n",
    "\n",
    "# Section 2\n",
    "from scipy import signal\n",
    "import scipy as sp\n",
    "\n",
    "# Sections 4-5\n",
    "from sklearn.model_selection import KFold, GridSearchCV, StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "# from sklearn.neural_network import MLPClassifier # use Tensorflow\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.metrics import ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Loading the data #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load and preprocess the data: copied from notebook 4 of exercises from week 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start loading ... \n",
      "\n",
      "================== Subject2 loaded. ==================\n",
      "================== Subject21 loaded. ==================\n",
      "Loading completed \n",
      "\n",
      "Start cutting data ... \n",
      "\n",
      "================== Working on Pig p2-t2 ==================\n",
      "================== Working on Pig p6-t6 ==================\n",
      "Data cutting completed \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# data_path = ''\n",
    "fs = 24400 # Hz\n",
    "\n",
    "# Subjects\n",
    "n_subjects = 2\n",
    "subjs_info_loading = {'Subject2' : {'Name' : 'p2-t2', 'Vars' : ['Baseline', 'Angio36', 'RR15', 'TV500']},\n",
    "                      'Subject21' : {'Name' : 'p6-t6', 'Vars' : ['Baseline', 'Angio36', 'RR15', 'TV125']}\n",
    "                     }\n",
    "subjs_info_final = {'Subject2' : {'Name' : 'p2-t2', 'Vars' : ['Baseline', 'Angio36', 'RRC', 'TVC']},\n",
    "                    'Subject21' : {'Name' : 'p6-t6', 'Vars' : ['Baseline', 'Angio36', 'RRC', 'TVC']}\n",
    "                   }\n",
    "animals_labels = ['p2-t2', 'p6-t6']\n",
    "\n",
    "def load_data_all_subjects(subjs_info_loading, subjs_info_final, fs, type_data = 'Field_Data_Neuro'):\n",
    "    subjects_names = list(subjs_info_loading.keys())\n",
    "    subjs_info = subjs_info_final\n",
    "    all_data = {}\n",
    "    print('Start loading ... \\n')\n",
    "    for subject in subjects_names:\n",
    "        name_subj, data_struct = load_data_one_subject(subject, subjs_info_loading, fs, type_data = type_data)\n",
    "        all_data[name_subj] = data_struct\n",
    "        print('================== %s loaded. =================='%subject)\n",
    "    return all_data\n",
    "\n",
    "\n",
    "def load_data_one_subject(subject, subjs_info_loading, fs, type_data = 'Field_Data_Neuro'):\n",
    "    name_subj_to_stock = subjs_info_loading[subject]['Name']\n",
    "    vars_to_load = subjs_info_loading[subject]['Vars']\n",
    "    data_struct = {}\n",
    "    \n",
    "    # file = h5py.File('data\\\\' + subject + '.mat','r')\n",
    "    file = h5py.File(subject + '.mat','r')\n",
    "    \n",
    "    # To get the names of the fields after decoding ASCII\n",
    "    all_field_names = get_field_names(file)\n",
    "\n",
    "    for var in vars_to_load:\n",
    "        id_field = np.where(all_field_names == var)[0]\n",
    "        curr_reference_data1 = file['Vagus_Data_Stimuli'][type_data][id_field][0][0]\n",
    "        curr_reference_data2 = file[curr_reference_data1][0][0]\n",
    "        final_data = np.transpose(np.asarray(file[curr_reference_data2]))\n",
    "        \n",
    "        if var == 'TV800' or var == 'TV500' or var == 'TV125': \n",
    "            var_name = 'TVC'\n",
    "        elif var == 'RR15' or var == 'RR20': \n",
    "            var_name = 'RRC'\n",
    "        else:\n",
    "            var_name = var\n",
    "            \n",
    "        data_struct[var_name] = {}\n",
    "        data_struct[var_name]['Data'] = final_data\n",
    "        n_time_pts = np.shape(final_data)[-1]\n",
    "        data_struct[var_name]['Time_pts'] = np.linspace(0, n_time_pts/fs, n_time_pts)\n",
    "        \n",
    "    return name_subj_to_stock, data_struct\n",
    "\n",
    "def get_field_names(file):\n",
    "    n_fields,_ = np.shape(file['Vagus_Data_Stimuli']['stimuli_name'])\n",
    "    all_field_names = []\n",
    "    for field in range(n_fields):\n",
    "        curr_reference_field = file['Vagus_Data_Stimuli']['stimuli_name'][field][0]\n",
    "        curr_field_ASCII = file[curr_reference_field]\n",
    "        curr_field = decode_ASCII(curr_field_ASCII)\n",
    "        all_field_names.append(curr_field)\n",
    "    return np.asarray(all_field_names)\n",
    "\n",
    "\n",
    "def decode_ASCII(numbers_array):\n",
    "    name = ''\n",
    "    squeezed_numbers = np.squeeze(numbers_array)\n",
    "    for n in squeezed_numbers:\n",
    "        name += chr(n)\n",
    "    return name\n",
    "\n",
    "data = load_data_all_subjects(subjs_info_loading, subjs_info_final, fs)\n",
    "print('Loading completed \\n')\n",
    "\n",
    "#print('Showing the data file:')\n",
    "#for key1 in data.keys():\n",
    "#    print('=========== %s ==========='%key1)\n",
    "#    for key2 in data[key1].keys():\n",
    "#        print('--- %s'%key2)\n",
    "#        for key3 in data[key1][key2].keys():\n",
    "#            print(' - %s'%key3)\n",
    "#            print('Shape : ', np.shape(data[key1][key2][key3]))\n",
    "\n",
    "def cut_all_data_to_established_duration_per_challenge(data):\n",
    "    '''\n",
    "    This function is a wrap-up to the function 'cut_data_one_pig_to_established_duration_per_challenge'.\n",
    "    '''\n",
    "    \n",
    "    new_data_struct = {}\n",
    "    for pig in data.keys():\n",
    "        print('================== Working on Pig %s =================='%pig)\n",
    "        data_curr_pig = data[pig]\n",
    "        data_curr_pig_cut = cut_data_one_pig_to_established_duration_per_challenge(data_curr_pig)\n",
    "        new_data_struct[pig] = data_curr_pig_cut\n",
    "        \n",
    "    return new_data_struct\n",
    "\n",
    "\n",
    "def cut_data_one_pig_to_established_duration_per_challenge(data_one_pig):\n",
    "    ''' \n",
    "    This function is used to cut the data for each challenge to the duration shown in Suppl. Table 1 in Vallone et al., 2021. \n",
    "    We take the first part of the data for each challenge (arbitrary choice). \n",
    "    '''\n",
    "    new_struct = copy.deepcopy(data_one_pig)\n",
    "    \n",
    "    dur_baseline = 5 #min\n",
    "    dur_RRC = 2 #min\n",
    "    dur_TVC = 2 #min\n",
    "    \n",
    "    for challenge in new_struct.keys():\n",
    "        data_curr_chal = new_struct[challenge]['Data']\n",
    "        time_pts_curr_chal = new_struct[challenge]['Time_pts']\n",
    "        t_end_sec = time_pts_curr_chal[-1]\n",
    "        \n",
    "        if challenge == 'Baseline': t_end_sec = dur_baseline * 60\n",
    "        elif challenge == 'RRC': t_end_sec = dur_RRC * 60\n",
    "        elif challenge == 'TVC': t_end_sec = dur_TVC * 60\n",
    "            \n",
    "#         print('Challenge %s , t_end_sec %0.3f'%(challenge, t_end_sec))\n",
    "            \n",
    "        id_t_end_curr_chal = find_specific_time_index(time_pts_curr_chal, t_end_sec)\n",
    "        new_struct[challenge]['Data'] = data_curr_chal[:,:id_t_end_curr_chal]\n",
    "        new_struct[challenge]['Time_pts'] = time_pts_curr_chal[:id_t_end_curr_chal]\n",
    "    \n",
    "    return new_struct\n",
    "\n",
    "def find_specific_time_index(time_pts, t):\n",
    "    t_id = np.argmin(np.abs(time_pts - t))\n",
    "    return t_id\n",
    "\n",
    "print('Start cutting data ... \\n')\n",
    "cut_data = cut_all_data_to_established_duration_per_challenge(data)\n",
    "print('Data cutting completed \\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data is structured pig -> challenge -> data, time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Showing the data files:\n",
      "=========== p2-t2 ===========\n",
      "--- Baseline\n",
      " - Data\n",
      "Shape :  (8, 7319999) , type:  <class 'numpy.ndarray'>\n",
      " - Time_pts\n",
      "Shape :  (7319999,) , type:  <class 'numpy.ndarray'>\n",
      "--- Angio36\n",
      " - Data\n",
      "Shape :  (8, 14780415) , type:  <class 'numpy.ndarray'>\n",
      " - Time_pts\n",
      "Shape :  (14780415,) , type:  <class 'numpy.ndarray'>\n",
      "--- RRC\n",
      " - Data\n",
      "Shape :  (8, 2927999) , type:  <class 'numpy.ndarray'>\n",
      " - Time_pts\n",
      "Shape :  (2927999,) , type:  <class 'numpy.ndarray'>\n",
      "--- TVC\n",
      " - Data\n",
      "Shape :  (8, 2927999) , type:  <class 'numpy.ndarray'>\n",
      " - Time_pts\n",
      "Shape :  (2927999,) , type:  <class 'numpy.ndarray'>\n",
      "=========== p6-t6 ===========\n",
      "--- Baseline\n",
      " - Data\n",
      "Shape :  (16, 7319999) , type:  <class 'numpy.ndarray'>\n",
      " - Time_pts\n",
      "Shape :  (7319999,) , type:  <class 'numpy.ndarray'>\n",
      "--- Angio36\n",
      " - Data\n",
      "Shape :  (16, 14940159) , type:  <class 'numpy.ndarray'>\n",
      " - Time_pts\n",
      "Shape :  (14940159,) , type:  <class 'numpy.ndarray'>\n",
      "--- RRC\n",
      " - Data\n",
      "Shape :  (16, 2927999) , type:  <class 'numpy.ndarray'>\n",
      " - Time_pts\n",
      "Shape :  (2927999,) , type:  <class 'numpy.ndarray'>\n",
      "--- TVC\n",
      " - Data\n",
      "Shape :  (16, 2927999) , type:  <class 'numpy.ndarray'>\n",
      " - Time_pts\n",
      "Shape :  (2927999,) , type:  <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print('Showing the data files:')\n",
    "for key1 in cut_data.keys():\n",
    "    print('=========== %s ==========='%key1)\n",
    "    for key2 in cut_data[key1].keys():\n",
    "        print('--- %s'%key2)\n",
    "        for key3 in cut_data[key1][key2].keys():\n",
    "            print(' - %s'%key3)\n",
    "            print('Shape : ', np.shape(cut_data[key1][key2][key3]), ', type: ', type(cut_data[key1][key2][key3]))\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Preprocessing #"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The paper says they use a 4-order Butterworth filter [1000, 6000] Hz. \n",
    "\n",
    "Note that we have fs = 24400 Hz, hence by Shannon the recorded signal can have no components above 2*fs 48000. \n",
    "\n",
    "Why do we bandpass at 6k Hz then???\n",
    "\n",
    "We highpass at 1kHz to remove any forms of noise, notably due to the organism itself. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_low = 1000 # [Hz]\n",
    "f_high = 6000 # [Hz]\n",
    "N = 4 # order of the filter\n",
    "sos = signal.butter(N, [f_low, f_high], 'bandpass', analog=False, fs=fs, output='sos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bandpass_data(data, sos):\n",
    "    print('Bandpassing the data... \\n')\n",
    "    new_data = copy.deepcopy(data)\n",
    "    for pig in data.keys():\n",
    "        print('=========== %s ==========='%pig)\n",
    "        for challenge in data[pig].keys():\n",
    "            new_data[pig][challenge] = signal.sosfilt(sos, cut_data[pig][challenge]['Data'])\n",
    "    return new_data        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that here we remove the 'time' field, which is not used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bandpassing the data... \n",
      "\n",
      "=========== p2-t2 ===========\n",
      "=========== p6-t6 ===========\n"
     ]
    }
   ],
   "source": [
    "# takes ~40sec to run\n",
    "data_filtered = bandpass_data(cut_data, sos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sub-sampling of the signal by a factor 2: \n",
    "\n",
    "(Why???)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes ~20sec to run\n",
    "data_subsampled = copy.deepcopy(data_filtered)\n",
    "# data_subsampled = data_blank\n",
    "for pig in data.keys():\n",
    "    for challenge in data[pig].keys():\n",
    "        data_subsampled[pig][challenge] = data_subsampled[pig][challenge][:,0:-1:2]\n",
    "\n",
    "data_final = data_subsampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data_preprocessed.npy', 'wb') as f:\n",
    "    np.save(f, data_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Split into test/train, perform windowing, extract features #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data_preprocessed.npy', 'rb') as f:\n",
    "    data_final = np.load(f, allow_pickle=True).item()\n",
    "fs = 24400"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following this we have the following data structure: four separate sets, each set contains the four challenges, for each challenge the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute features, as given in the paper. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the structure used:\n",
    "\n",
    "- compute_features computes features on the time series for a single channel, for a single challenge\n",
    "- windowing_and_features generates features for a given challenge; it iterates in time to create windows, inside this loop it iterates on each channel and concatenates the result to make a feature vector [(num_features*num_channels)]; \n",
    "- data_windowing_and_features creates the features array and the labels vector by combining the features on the four challenges\n",
    "\n",
    "Other implementation detail: it's easier to perform windowing and features computation in the same loop, because that way the dimension you gain from windowing is lost in computing features, then you don't need to use 3-d arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_features(data):\n",
    "    '''\n",
    "    :param data: vector [timesteps]\n",
    "    :return features: vector of features for that time series [9]\n",
    "    '''\n",
    "    mean = np.mean(data)\n",
    "    variance = np.var(data)\n",
    "    skew = sp.stats.skew(data)\n",
    "    kurtosis = sp.stats.kurtosis(data)\n",
    "    std = np.std(data)\n",
    "    mav = np.mean(np.absolute(data))\n",
    "    max = np.amax(data)\n",
    "    amp = 0 #\n",
    "    wl = 0  #\n",
    "    pow = np.sqrt(np.mean(data ** 2))\n",
    "\n",
    "    for i in range(len(data) - 1):\n",
    "        if abs(data[i + 1] - data[i]) >= std:\n",
    "            amp += 1\n",
    "        wl += abs(data[i + 1] - data[i])\n",
    "    \n",
    "    features = np.array([mean, variance, skew, kurtosis, mav, max, amp, wl, pow])\n",
    "    return features\n",
    "\n",
    "def windowing_and_features(data, ws, wo):\n",
    "    '''\n",
    "    :param data: ndarray [channels x timesteps]\n",
    "    :return: ndarray [windows x (features*channels)]\n",
    "    '''\n",
    "    windowStep_n = ws - wo\n",
    "    num_windows = int((data.shape[1]-ws)/windowStep_n)\n",
    "    num_features = 9\n",
    "    num_channels = data.shape[0]\n",
    "    features = np.empty((num_windows, num_features*num_channels))\n",
    "    for i in range(num_windows): # iterate over windows\n",
    "        for j in range(num_channels): # iterate over channels\n",
    "            window = data[j, i*windowStep_n:i*windowStep_n+ws]\n",
    "            features_channel = compute_features(window)\n",
    "            features[i,j*num_features:(j+1)*num_features] = features_channel\n",
    "    return features\n",
    "\n",
    "# def data_windowing_and_features(data, ws, wo):\n",
    "#     '''\n",
    "#     :param data: dictionary with four keys (challenges), each challenge contains an ndarray [channels x timesteps]\n",
    "#     :return features: ndarray [folds x (challenges*windows) x (features*channels)]\n",
    "#     :return labels: vector [folds x (challenges*windows)]\n",
    "#     '''\n",
    "\n",
    "#     ws = int(ws)\n",
    "#     wo = int(wo)\n",
    "\n",
    "#     kf = KFold(10)\n",
    "#     num_features = 9\n",
    "#     num_channels = data[list(data.keys())[0]].shape[0] # height of the first dict element\n",
    "\n",
    "#     train_features = np.array([])\n",
    "#     train_labels = np.array([])\n",
    "#     test_features = np.array([])\n",
    "#     test_labels = np.array([])\n",
    "\n",
    "#     for current_label, (challenge, values) in enumerate(data.items()):\n",
    "#         print(challenge)\n",
    "#         challenge_train_features = np.array([])\n",
    "#         challenge_train_labels = np.array([])\n",
    "#         challenge_test_features = np.array([])\n",
    "#         challenge_test_labels = np.array([])\n",
    "#         for fold_index, (train_index, test_index) in enumerate(kf.split(values.T)):\n",
    "#             print(train_index[0])\n",
    "#             print(test_index[0])\n",
    "#             #print(\"Train:\", train_index[0],\":\",train_index[-1], \"- Test:\", test_index[0],\":\",test_index[-1])\n",
    "#             train_data = values[:,train_index]\n",
    "#             test_data = values[:,test_index]\n",
    "#             current_train_features = windowing_and_features(train_data, ws, wo)\n",
    "#             current_train_labels = current_label * np.ones(current_train_features.shape[0])\n",
    "#             current_test_features = windowing_and_features(test_data, ws, wo)\n",
    "#             current_test_labels = current_label * np.ones(current_test_features.shape[0])\n",
    "#             # print(train_data.shape)\n",
    "#             # print(test_data.shape)\n",
    "#             # print(current_train_features.shape)\n",
    "#             # print(current_train_labels.shape)\n",
    "#             # print(current_test_features.shape)\n",
    "#             # print(current_test_labels.shape)\n",
    "#             challenge_train_features = np.dstack((challenge_train_features, current_train_features)) if challenge_train_features.size else current_train_features\n",
    "#             challenge_train_labels = np.vstack((challenge_train_labels, current_train_labels)) if challenge_train_labels.size else current_train_labels\n",
    "#             challenge_test_features = np.dstack((challenge_test_features, current_test_features)) if challenge_test_features.size else current_test_features\n",
    "#             challenge_test_labels = np.vstack((challenge_test_labels, current_test_labels)) if challenge_test_labels.size else current_test_labels\n",
    "\n",
    "#         train_features = np.vstack((train_features, challenge_train_features)) if train_features.size else challenge_train_features\n",
    "#         train_labels = np.vstack((train_labels, challenge_train_labels)) if train_labels.size else challenge_train_labels\n",
    "#         test_features = np.vstack((test_features, challenge_test_features)) if test_features.size else challenge_test_features\n",
    "#         test_labels = np.vstack((test_labels, challenge_test_labels)) if test_labels.size else challenge_test_labels\n",
    "\n",
    "#     return train_features, train_labels, test_features, test_labels\n",
    "\n",
    "def data_windowing_and_features(data, ws, wo):\n",
    "    '''\n",
    "    :param data: dictionary with four keys (challenges), each challenge contains an ndarray [channels x timesteps]\n",
    "    :return features: ndarray [folds x (challenges*windows) x (features*channels)]\n",
    "    :return labels: vector [folds x (challenges*windows)]\n",
    "    '''\n",
    "\n",
    "    ws = int(ws)\n",
    "    wo = int(wo)\n",
    "\n",
    "    kf = KFold(10)\n",
    "    num_features = 9\n",
    "    num_channels = data[list(data.keys())[0]].shape[0] # height of the first dict element\n",
    "\n",
    "    train_features = np.array([])\n",
    "    train_labels = np.array([])\n",
    "    test_features = np.array([])\n",
    "    test_labels = np.array([])\n",
    "\n",
    "    for current_label, (challenge, values) in enumerate(data.items()):\n",
    "        print(challenge)\n",
    "        # challenge_train_features = np.array([])\n",
    "        # challenge_train_labels = np.array([])\n",
    "        # challenge_test_features = np.array([])\n",
    "        # challenge_test_labels = np.array([])\n",
    "        # for fold_index, (train_index, test_index) in enumerate(kf.split(values.T)):\n",
    "        #     print(train_index[0])\n",
    "        #     print(test_index[0])\n",
    "        #     #print(\"Train:\", train_index[0],\":\",train_index[-1], \"- Test:\", test_index[0],\":\",test_index[-1])\n",
    "        #     train_data = values[:,train_index]\n",
    "        #     test_data = values[:,test_index]\n",
    "        current_train_features = windowing_and_features(values, ws, wo)\n",
    "        current_train_labels = current_label * np.ones(current_train_features.shape[0])\n",
    "        current_test_features = windowing_and_features(values, ws, 0)\n",
    "        current_test_labels = current_label * np.ones(current_test_features.shape[0])\n",
    "        #     # print(train_data.shape)\n",
    "        #     # print(test_data.shape)\n",
    "        print(\"current_train_features\", current_train_features.shape)\n",
    "        print(\"current_train_label\", current_train_labels.shape)\n",
    "        print(\"current_test_features\", current_test_features.shape)\n",
    "        print(\"current_test_labels\", current_test_labels.shape)\n",
    "        #     challenge_train_features = np.dstack((challenge_train_features, current_train_features)) if challenge_train_features.size else current_train_features\n",
    "        #     challenge_train_labels = np.vstack((challenge_train_labels, current_train_labels)) if challenge_train_labels.size else current_train_labels\n",
    "        #     challenge_test_features = np.dstack((challenge_test_features, current_test_features)) if challenge_test_features.size else current_test_features\n",
    "        #     challenge_test_labels = np.vstack((challenge_test_labels, current_test_labels)) if challenge_test_labels.size else current_test_labels\n",
    "\n",
    "        train_features = np.vstack((train_features, current_train_features)) if train_features.size else current_train_features\n",
    "        train_labels = np.concatenate((train_labels, current_train_labels)) if train_labels.size else current_train_labels\n",
    "        test_features = np.vstack((test_features, current_test_features)) if test_features.size else current_test_features\n",
    "        test_labels = np.concatenate((test_labels, current_test_labels)) if test_labels.size else current_test_labels\n",
    "        print(\"train_features\", train_features.shape)\n",
    "        print(\"train_labels\", train_labels.shape)\n",
    "        print(\"test_features\", test_features.shape)\n",
    "        print(\"test_labels\", test_labels.shape)\n",
    "\n",
    "    return train_features, train_labels, test_features, test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers_list = [RandomForestClassifier]\n",
    "window_sizes_t = np.linspace(0.05, 1, round(1/0.05)) #[s]\n",
    "window_sizes_n = np.around(window_sizes_t * fs) #[timesteps]\n",
    "window_overlaps_t = 0.2 * window_sizes_t #[s]\n",
    "window_overlaps_n = np.around(window_overlaps_t * fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#FOR TESTING\n",
    "ws = window_sizes_n[0]\n",
    "wo = window_overlaps_n[0]\n",
    "print(\"Ws:\", ws, \"| Wo:\", wo)\n",
    "train_features, train_labels, test_features, test_labels = data_windowing_and_features(data_final['p2-t2'], ws, wo)\n",
    "print(train_features.shape)\n",
    "print(train_labels.shape)\n",
    "print(test_features.shape)\n",
    "print(test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pig: p2-t2\n",
      "Ws: 1220.0 | Wo: 244.0\n",
      "Baseline\n"
     ]
    }
   ],
   "source": [
    "data_feature = {}\n",
    "for pig, pig_data in data_final.items():\n",
    "\tprint(\"Pig:\", pig)\n",
    "\tdata_feature[pig] = {}\n",
    "\tfor ws, wo in zip(window_sizes_n, window_overlaps_n):\n",
    "\t\tdata_feature[pig][ws] = {}\n",
    "\t\tprint(\"Ws:\", ws, \"| Wo:\", wo)\n",
    "\t\t#Extract windows and features\n",
    "\t\ttrain_features, train_labels, test_features, test_labels = data_windowing_and_features(pig_data, ws, wo)\n",
    "\t\tdata_feature[pig][ws][\"x_mean\"] = train_features.mean(axis=0)\n",
    "\t\tdata_feature[pig][ws][\"x_std\"] = train_features.std(axis=0)\n",
    "\t\tdata_feature[pig][ws][\"train_x_raw\"] = train_features\n",
    "\t\tdata_feature[pig][ws][\"train_x\"] = (train_features - data_feature[pig][ws][\"x_mean\"]) / data_feature[pig][ws][\"x_std\"]\n",
    "\t\tdata_feature[pig][ws][\"train_y\"] = train_labels\n",
    "\t\tdata_feature[pig][ws][\"test_x_raw\"] = test_features\n",
    "\t\tdata_feature[pig][ws][\"test_x\"] = (test_features - data_feature[pig][ws][\"x_mean\"]) / data_feature[pig][ws][\"x_std\"]\n",
    "\t\tdata_feature[pig][ws][\"test_y\"] = test_labels\n",
    "\t\t# print(data_feature[pig][ws][\"train_x\"].mean(axis=0))\n",
    "\t\t# print(data_feature[pig][ws][\"train_x\"].std(axis=0))\n",
    "\t\t# print(train_features.shape)\n",
    "\t\t# print(train_labels.shape)\n",
    "\t\t# print(test_features.shape)\n",
    "\t\t# print(test_labels.shape)\n",
    "\t\t#Grid search training with cross validation\n",
    "\t\t#Final model validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data_features.npy', 'wb') as f:\n",
    "    np.save(f, data_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_feature_back = data_feature\n",
    "with open('data_features.npy', 'rb') as f:\n",
    "    data_feature = np.load(f, allow_pickle=True).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.18946728 -1.18946728 -1.18946728 ...  2.20217309  2.20217309\n",
      "  2.20217309]\n"
     ]
    }
   ],
   "source": [
    "print(data_feature['p2-t2'][1220][\"train_y\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating training data for pig 1...\n",
      "Finished generating training data for pig 1\n",
      "Generating testing data for pig 1...\n",
      "Finished generating testing data for pig 1\n",
      "Generating training data for pig 2...\n",
      "Finished generating training data for pig 2\n",
      "Generating testing data for pig 2...\n",
      "Finished generating testing data for pig 2\n"
     ]
    }
   ],
   "source": [
    "# [TODO] to find the best ws, several sizes were tested from ws = 50ms to ws = 1s with steps of 50 ms.\n",
    "\n",
    "windowSize_t = 50 # [ms]\n",
    "windowSize_n = int(windowSize_t * fs * 0.001) #[timesteps]\n",
    "windowOverlap_train_t = 20 #[ms]\n",
    "windowOverlap_train_n = int(windowOverlap_train_t * fs * 0.001)\n",
    "windowOverlap_test_n = 0\n",
    "\n",
    "print('Generating training data for pig 1...')\n",
    "pig1_train_X, pig1_train_y = data_windowing_and_features(pig1_train_data, windowSize_n, windowOverlap_train_n)\n",
    "print('Finished generating training data for pig 1')\n",
    "print('Generating testing data for pig 1...')\n",
    "pig1_test_X, pig1_test_y = data_windowing_and_features(pig1_test_data, windowSize_n, windowOverlap_test_n)\n",
    "print('Finished generating testing data for pig 1')\n",
    "\n",
    "print('Generating training data for pig 2...')\n",
    "pig2_train_X, pig2_train_y = data_windowing_and_features(pig2_train_data, windowSize_n, windowOverlap_train_n)\n",
    "print('Finished generating training data for pig 2')\n",
    "print('Generating testing data for pig 2...')\n",
    "pig2_test_X, pig2_test_y = data_windowing_and_features(pig2_test_data, windowSize_n, windowOverlap_test_n)\n",
    "print('Finished generating testing data for pig 2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The vectors look fine at a first glance ... (note that pig2 has twice as many recording channels so the features vector is two times longer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17178, 72)\n",
      "(17178,)\n",
      "(1141, 72)\n",
      "(1141,)\n",
      "(17276, 144)\n",
      "(17276,)\n",
      "(1148, 144)\n",
      "(1148,)\n"
     ]
    }
   ],
   "source": [
    "print(pig1_train_X.shape)\n",
    "print(pig1_train_y.shape)\n",
    "print(pig1_test_X.shape)\n",
    "print(pig1_test_y.shape)\n",
    "print(pig2_train_X.shape)\n",
    "print(pig2_train_y.shape)\n",
    "print(pig2_test_X.shape)\n",
    "print(pig2_test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2.06182545e-09  1.01400968e-11 -7.28075476e-02 ...  3.09000000e+02\n",
      "   2.21830473e-03  2.49513404e-06]\n",
      " [-5.62823297e-09  1.02704590e-11 -3.41194766e-02 ...  3.22000000e+02\n",
      "   2.23127705e-03  2.47020535e-06]\n",
      " [ 6.50087471e-10  1.04629099e-11 -8.80026964e-03 ...  3.40000000e+02\n",
      "   2.19553394e-03  2.41157685e-06]\n",
      " ...\n",
      " [-2.76207252e-09  1.21895792e-11 -1.24975353e-01 ...  3.33000000e+02\n",
      "   2.27811433e-03  2.54226255e-06]\n",
      " [ 3.58589470e-10  1.13805201e-11  5.57459530e-02 ...  3.53000000e+02\n",
      "   2.25277658e-03  2.42579245e-06]\n",
      " [ 2.39236873e-09  1.23116876e-11  1.07057651e-01 ...  3.24000000e+02\n",
      "   2.19928159e-03  2.40340710e-06]]\n"
     ]
    }
   ],
   "source": [
    "print(pig1_train_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. ... 3. 3. 3.]\n"
     ]
    }
   ],
   "source": [
    "print(pig1_test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. RF Classifier #"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hyperparameters to vary for each window size:\n",
    "\n",
    "\"During training, a grid-search was performed in which the number of trees varied in [50, 100, 200, 400], while their max- imal depth in [5, 10, 20]\"\n",
    "\n",
    "Make the previous section a function so we can test different window sizes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_tt(data, windowSize_t, fs):\n",
    "    '''\n",
    "    :param data: dict of arrays, ie data for one pig\n",
    "    '''\n",
    "    tt_split = 0.9 # magic number from paper\n",
    "    train_data, test_data = split_data(data, tt_split)\n",
    "\n",
    "    windowSize_n = int(windowSize_t * fs * 0.001) #[timesteps]\n",
    "    windowOverlap_train_t = 20 #[ms] # magic number form paper\n",
    "    windowOverlap_train_n = int(windowOverlap_train_t * fs * 0.001)\n",
    "    windowOverlap_test_n = 0 # magic number form paper\n",
    "    train_X, train_y = data_windowing_and_features(train_data, windowSize_n, windowOverlap_train_n)\n",
    "    test_X, test_y = data_windowing_and_features(test_data, windowSize_n, windowOverlap_test_n)\n",
    "    return train_X, train_y, test_X, test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pig1 = 'p2-t2'\n",
    "pig2 = 'p6-t6'\n",
    "\n",
    "for windowSize_t in np.arange(50, 1000, 50): # [ms]\n",
    "    print('Window size: %.i ms'%windowSize_t)\n",
    "    pig1_train_X, pig1_train_y, pig1_test_X, pig1_test_y = generate_tt(data_subsampled[pig1], windowSize_t, fs)\n",
    "    pig2_train_X, pig2_train_y, pig2_test_X, pig2_test_y = generate_tt(data_subsampled[pig2], windowSize_t, fs)\n",
    "    # [TODO]: fill in the ML\n",
    "    class_weights = {}\n",
    "    clf = RandomForestClassifier(n_estimators=100, max_depth=None)\n",
    "    clf.fit(pig1_train_X, pig1_train_y)\n",
    "    pig1_predicted_y = clf.predict(pig1_test_X)\n",
    "\n",
    "    clf.fit(pig2_train_X, pig2_train_y)\n",
    "    pig2_predicted_y = clf.predict(pig2_test_X)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Other Classifiers # "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLP\n",
    "\n",
    "hyperparameters to vary for each window size:\n",
    "\n",
    "\"for multi layer perceptron (MLP), the three architectures [128, 128, 128, 128], [128, 512, 512, 128], and [128, 256, 512, 512] were tested, where the numbers indicate the number of hidden neurons from the first to the fourth layer. For MLP, training was performed with a learning rate of 1 × 10−3 for 30 epochs and a batch size of 128\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for windowSize_t in np.arange(50, 1000, 50): # [ms]\n",
    "    print('Window size: %.i ms'%windowSize_t)\n",
    "    pig1_train_X, pig1_train_y, pig1_test_X, pig1_test_y = generate_tt(data_subsampled[pig1], windowSize_t, fs)\n",
    "    pig2_train_X, pig2_train_y, pig2_test_X, pig2_test_y = generate_tt(data_subsampled[pig2], windowSize_t, fs)\n",
    "    # [TODO]: fill in the ML\n",
    "    # Should use tensorflow"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVM\n",
    "\n",
    "hyperparameters to vary for each window size:\n",
    "\n",
    "\"for SVM, the kernel function was selected between the radial basis function (rbf) function or the sigmoid\n",
    "function and the L2 regularization value varied in [1 × 10−2 , 1 × 10−1 , 1], with 1 indicating no regularization\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BAC_pig1 = np.empty(0)\n",
    "BAC_pig2 = np.empty(0)\n",
    "for windowSize_t in np.arange(50, 1000, 50): # [ms]\n",
    "    print('Window size: %.i ms'%windowSize_t)\n",
    "    pig1_train_X, pig1_train_y, pig1_test_X, pig1_test_y = generate_tt(data_subsampled[pig1], windowSize_t, fs)\n",
    "    # Adapted from: https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html\n",
    "    clf = make_pipeline(StandardScaler(), SVC(gamma='auto'))\n",
    "    clf.fit(pig1_train_X, pig1_train_y)\n",
    "    pig1_predicted_y = clf.predict(pig1_test_X)\n",
    "    BAC1 = balanced_accuracy_score(pig1_test_y, pig1_predicted_y)\n",
    "    BAC_pig1.append(BAC1)\n",
    "    \n",
    "    pig2_train_X, pig2_train_y, pig2_test_X, pig2_test_y = generate_tt(data_subsampled[pig2], windowSize_t, fs)\n",
    "    clf.fit(pig2_train_X, pig2_train_y)\n",
    "    pig2_predicted_y = clf.predict(pig2_test_X)\n",
    "    BAC2 = balanced_accuracy_score(pig2_test_y, pig2_predicted_y)\n",
    "    BAC_pig2.append(BAC2)\n",
    "\n",
    "plt.plot(np.arange(50, 1000, 50), BAC_pig1)\n",
    "plt.plot(np.arange(50, 1000, 50), BAC_pig2)\n",
    "plt.xlabel('Time window length [ms]')\n",
    "plt.ylabel('Balanced Accuracy [%]')\n",
    "plt.legend('p2-t2', 'p6-t6')\n",
    "plt.show()\n",
    "\n",
    "windowSize_t = 500 #[ms]\n",
    "pig1_train_X, pig1_train_y, pig1_test_X, pig1_test_y = generate_tt(data_subsampled[pig1], windowSize_t, fs)\n",
    "clf = make_pipeline(StandardScaler(), SVC(gamma='auto'))\n",
    "clf.fit(pig1_train_X, pig1_train_y)\n",
    "# The forum post below gives code to generate confusion matrices exactly like in the paper:\n",
    "# https://stackoverflow.com/questions/40264763/how-can-i-make-my-confusion-matrix-plot-only-1-decimal-in-python\n",
    "ConfusionMatrixDisplay.from_estimator(clf, pig1_test_X, pig1_test_y)\n",
    "plt.show()\n",
    "\n",
    "pig2_train_X, pig2_train_y, pig2_test_X, pig2_test_y = generate_tt(data_subsampled[pig2], windowSize_t, fs)\n",
    "clf.fit(pig2_train_X, pig2_train_y)\n",
    "ConfusionMatrixDisplay.from_estimator(clf, pig2_test_X, pig2_test_y)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-Means\n",
    "\n",
    "hyperparameters to vary for each window size:\n",
    "\n",
    "\"for KNN, the number of neighbors varied in [3, 5, 7, 9, 11, 13]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for windowSize_t in np.arange(50, 1000, 50): # [ms]\n",
    "    print('Window size: %.i ms'%windowSize_t)\n",
    "    pig1_train_X, pig1_train_y, pig1_test_X, pig1_test_y = generate_tt(data_subsampled[pig1], windowSize_t, fs)\n",
    "    pig2_train_X, pig2_train_y, pig2_test_X, pig2_test_y = generate_tt(data_subsampled[pig2], windowSize_t, fs)\n",
    "    # [TODO]: fill in the ML"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "81794d4967e6c3204c66dcd87b604927b115b27c00565d3d43f05ba2f3a2cb0d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
