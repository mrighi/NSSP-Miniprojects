{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 1\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "import copy\n",
    "\n",
    "# Section 2\n",
    "from scipy import signal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Loading the data #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load and preprocess the data: copied from notebook 4 of exercises from week 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start loading ... \n",
      "\n",
      "================== Subject2 loaded. ==================\n",
      "================== Subject21 loaded. ==================\n",
      "Loading completed \n",
      "\n",
      "Start cutting data ... \n",
      "\n",
      "================== Working on Pig p2-t2 ==================\n",
      "================== Working on Pig p6-t6 ==================\n",
      "Data cutting completed \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# data_path = ''\n",
    "fs = 24400 # Hz\n",
    "\n",
    "# Subjects\n",
    "n_subjects = 2\n",
    "subjs_info_loading = {'Subject2' : {'Name' : 'p2-t2', 'Vars' : ['Baseline', 'Angio36', 'RR15', 'TV500']},\n",
    "                      'Subject21' : {'Name' : 'p6-t6', 'Vars' : ['Baseline', 'Angio36', 'RR15', 'TV125']}\n",
    "                     }\n",
    "subjs_info_final = {'Subject2' : {'Name' : 'p2-t2', 'Vars' : ['Baseline', 'Angio36', 'RRC', 'TVC']},\n",
    "                    'Subject21' : {'Name' : 'p6-t6', 'Vars' : ['Baseline', 'Angio36', 'RRC', 'TVC']}\n",
    "                   }\n",
    "animals_labels = ['p2-t2', 'p6-t6']\n",
    "\n",
    "def load_data_all_subjects(subjs_info_loading, subjs_info_final, fs, type_data = 'Field_Data_Neuro'):\n",
    "    subjects_names = list(subjs_info_loading.keys())\n",
    "    subjs_info = subjs_info_final\n",
    "    all_data = {}\n",
    "    print('Start loading ... \\n')\n",
    "    for subject in subjects_names:\n",
    "        name_subj, data_struct = load_data_one_subject(subject, subjs_info_loading, fs, type_data = type_data)\n",
    "        all_data[name_subj] = data_struct\n",
    "        print('================== %s loaded. =================='%subject)\n",
    "    return all_data\n",
    "\n",
    "\n",
    "def load_data_one_subject(subject, subjs_info_loading, fs, type_data = 'Field_Data_Neuro'):\n",
    "    name_subj_to_stock = subjs_info_loading[subject]['Name']\n",
    "    vars_to_load = subjs_info_loading[subject]['Vars']\n",
    "    data_struct = {}\n",
    "    \n",
    "    # file = h5py.File('data\\\\' + subject + '.mat','r')\n",
    "    file = h5py.File(subject + '.mat','r')\n",
    "    \n",
    "    # To get the names of the fields after decoding ASCII\n",
    "    all_field_names = get_field_names(file)\n",
    "\n",
    "    for var in vars_to_load:\n",
    "        id_field = np.where(all_field_names == var)[0]\n",
    "        curr_reference_data1 = file['Vagus_Data_Stimuli'][type_data][id_field][0][0]\n",
    "        curr_reference_data2 = file[curr_reference_data1][0][0]\n",
    "        final_data = np.transpose(np.asarray(file[curr_reference_data2]))\n",
    "        \n",
    "        if var == 'TV800' or var == 'TV500' or var == 'TV125': \n",
    "            var_name = 'TVC'\n",
    "        elif var == 'RR15' or var == 'RR20': \n",
    "            var_name = 'RRC'\n",
    "        else:\n",
    "            var_name = var\n",
    "            \n",
    "        data_struct[var_name] = {}\n",
    "        data_struct[var_name]['Data'] = final_data\n",
    "        n_time_pts = np.shape(final_data)[-1]\n",
    "        data_struct[var_name]['Time_pts'] = np.linspace(0, n_time_pts/fs, n_time_pts)\n",
    "        \n",
    "    return name_subj_to_stock, data_struct\n",
    "\n",
    "def get_field_names(file):\n",
    "    n_fields,_ = np.shape(file['Vagus_Data_Stimuli']['stimuli_name'])\n",
    "    all_field_names = []\n",
    "    for field in range(n_fields):\n",
    "        curr_reference_field = file['Vagus_Data_Stimuli']['stimuli_name'][field][0]\n",
    "        curr_field_ASCII = file[curr_reference_field]\n",
    "        curr_field = decode_ASCII(curr_field_ASCII)\n",
    "        all_field_names.append(curr_field)\n",
    "    return np.asarray(all_field_names)\n",
    "\n",
    "\n",
    "def decode_ASCII(numbers_array):\n",
    "    name = ''\n",
    "    squeezed_numbers = np.squeeze(numbers_array)\n",
    "    for n in squeezed_numbers:\n",
    "        name += chr(n)\n",
    "    return name\n",
    "\n",
    "data = load_data_all_subjects(subjs_info_loading, subjs_info_final, fs)\n",
    "print('Loading completed \\n')\n",
    "\n",
    "#print('Showing the data file:')\n",
    "#for key1 in data.keys():\n",
    "#    print('=========== %s ==========='%key1)\n",
    "#    for key2 in data[key1].keys():\n",
    "#        print('--- %s'%key2)\n",
    "#        for key3 in data[key1][key2].keys():\n",
    "#            print(' - %s'%key3)\n",
    "#            print('Shape : ', np.shape(data[key1][key2][key3]))\n",
    "\n",
    "def cut_all_data_to_established_duration_per_challenge(data):\n",
    "    '''\n",
    "    This function is a wrap-up to the function 'cut_data_one_pig_to_established_duration_per_challenge'.\n",
    "    '''\n",
    "    \n",
    "    new_data_struct = {}\n",
    "    for pig in data.keys():\n",
    "        print('================== Working on Pig %s =================='%pig)\n",
    "        data_curr_pig = data[pig]\n",
    "        data_curr_pig_cut = cut_data_one_pig_to_established_duration_per_challenge(data_curr_pig)\n",
    "        new_data_struct[pig] = data_curr_pig_cut\n",
    "        \n",
    "    return new_data_struct\n",
    "\n",
    "\n",
    "def cut_data_one_pig_to_established_duration_per_challenge(data_one_pig):\n",
    "    ''' \n",
    "    This function is used to cut the data for each challenge to the duration shown in Suppl. Table 1 in Vallone et al., 2021. \n",
    "    We take the first part of the data for each challenge (arbitrary choice). \n",
    "    '''\n",
    "    new_struct = copy.deepcopy(data_one_pig)\n",
    "    \n",
    "    dur_baseline = 5 #min\n",
    "    dur_RRC = 2 #min\n",
    "    dur_TVC = 2 #min\n",
    "    \n",
    "    for challenge in new_struct.keys():\n",
    "        data_curr_chal = new_struct[challenge]['Data']\n",
    "        time_pts_curr_chal = new_struct[challenge]['Time_pts']\n",
    "        t_end_sec = time_pts_curr_chal[-1]\n",
    "        \n",
    "        if challenge == 'Baseline': t_end_sec = dur_baseline * 60\n",
    "        elif challenge == 'RRC': t_end_sec = dur_RRC * 60\n",
    "        elif challenge == 'TVC': t_end_sec = dur_TVC * 60\n",
    "            \n",
    "#         print('Challenge %s , t_end_sec %0.3f'%(challenge, t_end_sec))\n",
    "            \n",
    "        id_t_end_curr_chal = find_specific_time_index(time_pts_curr_chal, t_end_sec)\n",
    "        new_struct[challenge]['Data'] = data_curr_chal[:,:id_t_end_curr_chal]\n",
    "        new_struct[challenge]['Time_pts'] = time_pts_curr_chal[:id_t_end_curr_chal]\n",
    "    \n",
    "    return new_struct\n",
    "\n",
    "def find_specific_time_index(time_pts, t):\n",
    "    t_id = np.argmin(np.abs(time_pts - t))\n",
    "    return t_id\n",
    "\n",
    "print('Start cutting data ... \\n')\n",
    "cut_data = cut_all_data_to_established_duration_per_challenge(data)\n",
    "print('Data cutting completed \\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data is structured pig -> challenge -> data, time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Showing the data files:\n",
      "=========== p2-t2 ===========\n",
      "--- Baseline\n",
      " - Data\n",
      "Shape :  (8, 7319999) , type:  <class 'numpy.ndarray'>\n",
      " - Time_pts\n",
      "Shape :  (7319999,) , type:  <class 'numpy.ndarray'>\n",
      "--- Angio36\n",
      " - Data\n",
      "Shape :  (8, 14780415) , type:  <class 'numpy.ndarray'>\n",
      " - Time_pts\n",
      "Shape :  (14780415,) , type:  <class 'numpy.ndarray'>\n",
      "--- RRC\n",
      " - Data\n",
      "Shape :  (8, 2927999) , type:  <class 'numpy.ndarray'>\n",
      " - Time_pts\n",
      "Shape :  (2927999,) , type:  <class 'numpy.ndarray'>\n",
      "--- TVC\n",
      " - Data\n",
      "Shape :  (8, 2927999) , type:  <class 'numpy.ndarray'>\n",
      " - Time_pts\n",
      "Shape :  (2927999,) , type:  <class 'numpy.ndarray'>\n",
      "=========== p6-t6 ===========\n",
      "--- Baseline\n",
      " - Data\n",
      "Shape :  (16, 7319999) , type:  <class 'numpy.ndarray'>\n",
      " - Time_pts\n",
      "Shape :  (7319999,) , type:  <class 'numpy.ndarray'>\n",
      "--- Angio36\n",
      " - Data\n",
      "Shape :  (16, 14940159) , type:  <class 'numpy.ndarray'>\n",
      " - Time_pts\n",
      "Shape :  (14940159,) , type:  <class 'numpy.ndarray'>\n",
      "--- RRC\n",
      " - Data\n",
      "Shape :  (16, 2927999) , type:  <class 'numpy.ndarray'>\n",
      " - Time_pts\n",
      "Shape :  (2927999,) , type:  <class 'numpy.ndarray'>\n",
      "--- TVC\n",
      " - Data\n",
      "Shape :  (16, 2927999) , type:  <class 'numpy.ndarray'>\n",
      " - Time_pts\n",
      "Shape :  (2927999,) , type:  <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print('Showing the data files:')\n",
    "for key1 in cut_data.keys():\n",
    "    print('=========== %s ==========='%key1)\n",
    "    for key2 in cut_data[key1].keys():\n",
    "        print('--- %s'%key2)\n",
    "        for key3 in cut_data[key1][key2].keys():\n",
    "            print(' - %s'%key3)\n",
    "            print('Shape : ', np.shape(cut_data[key1][key2][key3]), ', type: ', type(cut_data[key1][key2][key3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Preprocessing #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The paper says they use a 4-order Butterworth filter [1000, 6000] Hz. \n",
    "\n",
    "Note that we have fs= 24400Hz, hence by Shannon the recorded signal can have no components above 2*fs 48000. \n",
    "\n",
    "Why do we bandpass at 6k Hz then???\n",
    "\n",
    "We highpass at 1kHz to remove any forms of noise, notably due to the organism itself. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_low = 1000 # [Hz]\n",
    "f_high = 6000 # [Hz]\n",
    "N = 4 # order of the filter\n",
    "sos = signal.butter(N, [f_low, f_high], 'bandpass', analog=False, fs=fs, output='sos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bandpass_data(data, sos):\n",
    "    print('Bandpassing the data... \\n')\n",
    "    new_data = copy.deepcopy(data)\n",
    "    for pig in data.keys():\n",
    "        print('=========== %s ==========='%pig)\n",
    "        for challenge in data[pig].keys():\n",
    "            new_data[pig][challenge] = signal.sosfilt(sos, cut_data[pig][challenge]['Data'])\n",
    "    return new_data        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that here we remove the 'time' field, which is not used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bandpassing the data... \n",
      "\n",
      "=========== p2-t2 ===========\n",
      "=========== p6-t6 ===========\n"
     ]
    }
   ],
   "source": [
    "# takes ~40sec to run\n",
    "data_filtered = bandpass_data(cut_data, sos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sub-sampling of the signal by a factor 2: \n",
    "\n",
    "(Why???)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes ~20sec to run\n",
    "data_subsampled = copy.deepcopy(data_filtered)\n",
    "# data_subsampled = data_blank\n",
    "for pig in data.keys():\n",
    "    for challenge in data[pig].keys():\n",
    "        data_subsampled[pig][challenge] = data_subsampled[pig][challenge][:,0:-1:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Split into test/train, perform windowing, extract features #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following the paper, we use a 90-10 train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(data, tt_split):\n",
    "    data_train = {'Baseline': {}, 'Angio36' : {}, 'RRC': {}, 'TVC': {}}\n",
    "    data_test = dict.fromkeys(data.keys(), {})\n",
    "    for challenge in data.keys():\n",
    "        L = data[challenge].shape[1]\n",
    "        data_train[challenge]= data[challenge][:,0:int(tt_split*L)]\n",
    "        data_test[challenge]= data[challenge][:,int(tt_split*L):]\n",
    "    return data_train, data_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Could make a dict of dicts but for two pigs it's unecessary\n",
    "tt_split = 0.9\n",
    "pig1 = 'p2-t2'\n",
    "pig2 = 'p6-t6'\n",
    "pig1_train_data, pig1_test_data = split_data(data_subsampled[pig1], tt_split)\n",
    "pig2_train_data, pig2_test_data = split_data(data_subsampled[pig2], tt_split)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following this we have the following data structure: four separate sets, each set contains the four challenges, for each challenge the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute features, as given in the paper. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the structure used:\n",
    "\n",
    "- compute_features computes features on the time series for a single channel, for a single challenge\n",
    "- windowing_and_features generates features for a given challenge; it iterates in time to create windows, inside this loop it iterates on each channel and concatenates the result to make a feature vector [(num_features*num_channels)]; \n",
    "- data_windowing_and_features creates the features array the labels vector by combining the features on the four challenges\n",
    "\n",
    "Other implementation detail: it's easier to perform windowing and features computation in the same function, because that way the dimension you gain from windiwing is lost in computing features, then you don't need to use 3-d arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating training data for pig 1...\n",
      "Baseline\n",
      "Angio36\n",
      "RRC\n",
      "TVC\n",
      "Finished generating training data for pig 1\n",
      "Generating testing data for pig 1...\n",
      "Baseline\n",
      "Angio36\n",
      "RRC\n",
      "TVC\n",
      "Finished generating testing data for pig 1\n",
      "Generating training data for pig 2...\n",
      "Baseline\n",
      "Angio36\n",
      "RRC\n",
      "TVC\n",
      "Finished generating training data for pig 2\n",
      "Generating testing data for pig 2...\n",
      "Baseline\n",
      "Angio36\n",
      "RRC\n",
      "TVC\n",
      "Finished generating testing data for pig 2\n"
     ]
    }
   ],
   "source": [
    "def compute_features(data):\n",
    "    # [TODO]\n",
    "    '''\n",
    "    :param data: vector [timesteps]\n",
    "    :return features: vector of features for that time series [9]\n",
    "    '''\n",
    "    mean = np.mean(data)\n",
    "    variance = np.var(data)\n",
    "    skew = 0\n",
    "    kurtosis = 0\n",
    "    mav = 0\n",
    "    max = 0\n",
    "    amp = 0\n",
    "    wh = 0\n",
    "    pow = 0\n",
    "    features = np.array([mean, variance, skew, kurtosis, mav, max, amp, wh, pow])\n",
    "    return features\n",
    "\n",
    "def windowing_and_features(data, windowSize_n, windowOverlap_n):\n",
    "    '''\n",
    "    :param data: ndarray [channels x timesteps]\n",
    "    :return: ndarray [windows x (features*channels)]\n",
    "    '''\n",
    "    windowStep_n = windowSize_n - windowOverlap_n\n",
    "    num_windows = int((data.shape[1]-windowSize_n)/windowStep_n)\n",
    "    num_features = 9\n",
    "    num_channels = data.shape[0]\n",
    "    features = np.empty((num_windows, num_features*num_channels))\n",
    "    for i in range(num_windows): # iterate over windows\n",
    "        for j in range(num_channels): # iterate over channels\n",
    "            window = data[j, i*windowStep_n:i*windowStep_n+windowSize_n]\n",
    "            features_channel = compute_features(window)\n",
    "            features[i,j*num_features:(j+1)*num_features] = features_channel\n",
    "    return features\n",
    "\n",
    "def data_windowing_and_features(data, windowSize_n, windowOverlap_n):\n",
    "    '''\n",
    "    :param data: dictionnary with four keys (challenges), each challenge contains an ndarray [channels x timesteps]\n",
    "    :return features: ndarray [(channels*windows) x (features*channels)]\n",
    "    :return labels: vector [(channels*windows)]\n",
    "    '''\n",
    "    num_features = 9\n",
    "    num_channels = data[list(data.keys())[0]].shape[0] # height of the first dict element\n",
    "    features = np.empty((0,num_features*num_channels))\n",
    "    labels = np.empty((0))\n",
    "    for challenge in data.keys():\n",
    "        print(challenge)\n",
    "        challenge_features = windowing_and_features(data[challenge], windowSize_n, windowOverlap_n)\n",
    "        label = list(data).index(challenge) # converts to an index\n",
    "        challenge_labels = label* np.ones(challenge_features.shape[0])\n",
    "        features = np.append(features, challenge_features, axis = 0)\n",
    "        labels = np.append(labels, challenge_labels)\n",
    "    return features, labels\n",
    "\n",
    "windowSize_t = 50 # [ms]\n",
    "windowSize_n = int(windowSize_t * fs * 0.001) #[timesteps]\n",
    "windowOverlap_train_t = 20 #[ms]\n",
    "windowOverlap_train_n = int(windowOverlap_train_t * fs * 0.001)\n",
    "windowOverlap_test_n = 0\n",
    "\n",
    "print('Generating training data for pig 1...')\n",
    "pig1_train_X, pig1_train_y = data_windowing_and_features(pig1_train_data, windowSize_n, windowOverlap_train_n)\n",
    "print('Finished generating training data for pig 1')\n",
    "print('Generating testing data for pig 1...')\n",
    "pig1_test_X, pig1_test_y = data_windowing_and_features(pig1_test_data, windowSize_n, windowOverlap_test_n)\n",
    "print('Finished generating testing data for pig 1')\n",
    "\n",
    "print('Generating training data for pig 2...')\n",
    "pig2_train_X, pig2_train_y = data_windowing_and_features(pig2_train_data, windowSize_n, windowOverlap_train_n)\n",
    "print('Finished generating training data for pig 2')\n",
    "print('Generating testing data for pig 2...')\n",
    "pig2_test_X, pig2_test_y = data_windowing_and_features(pig2_test_data, windowSize_n, windowOverlap_test_n)\n",
    "print('Finished generating testing data for pig 2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The vectors look fine at a first glance ... (note that pig2 has twice as many recording channels so the features vector is two times longer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17178, 72)\n",
      "(17178,)\n",
      "(1141, 72)\n",
      "(1141,)\n",
      "(17276, 144)\n",
      "(17276,)\n",
      "(1148, 144)\n",
      "(1148,)\n"
     ]
    }
   ],
   "source": [
    "print(pig1_train_X.shape)\n",
    "print(pig1_train_y.shape)\n",
    "print(pig1_test_X.shape)\n",
    "print(pig1_test_y.shape)\n",
    "print(pig2_train_X.shape)\n",
    "print(pig2_train_y.shape)\n",
    "print(pig2_test_X.shape)\n",
    "print(pig2_test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2.06182545e-09  1.01400968e-11  0.00000000e+00 ...  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00]\n",
      " [-5.62823297e-09  1.02704590e-11  0.00000000e+00 ...  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00]\n",
      " [ 6.50087471e-10  1.04629099e-11  0.00000000e+00 ...  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00]\n",
      " ...\n",
      " [-2.76207252e-09  1.21895792e-11  0.00000000e+00 ...  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00]\n",
      " [ 3.58589470e-10  1.13805201e-11  0.00000000e+00 ...  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00]\n",
      " [ 2.39236873e-09  1.23116876e-11  0.00000000e+00 ...  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "print(pig1_train_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. ... 3. 3. 3.]\n"
     ]
    }
   ],
   "source": [
    "print(pig1_test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. RF Classifier # "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Other Classifiers #"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "548cb84f3c3c3a027d09fd18aa2745d959a35cd8c151ed2c8b5f74a76b0084e1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
