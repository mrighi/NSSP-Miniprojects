{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Section 1\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "import copy\n",
    "\n",
    "# Section 2\n",
    "from scipy import signal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Loading the data #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load and preprocess the data: copied from notebook 4 of exercises from week 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start loading ... \n",
      "\n",
      "================== Subject2 loaded. ==================\n",
      "================== Subject21 loaded. ==================\n",
      "================== Working on Pig p2-t2 ==================\n",
      "================== Working on Pig p6-t6 ==================\n"
     ]
    }
   ],
   "source": [
    "# data_path = ''\n",
    "fs = 24400 # Hz\n",
    "\n",
    "# Subjects\n",
    "n_subjects = 2\n",
    "subjs_info_loading = {'Subject2' : {'Name' : 'p2-t2', 'Vars' : ['Baseline', 'Angio36', 'RR15', 'TV500']},\n",
    "                      'Subject21' : {'Name' : 'p6-t6', 'Vars' : ['Baseline', 'Angio36', 'RR15', 'TV125']}\n",
    "                     }\n",
    "subjs_info_final = {'Subject2' : {'Name' : 'p2-t2', 'Vars' : ['Baseline', 'Angio36', 'RRC', 'TVC']},\n",
    "                    'Subject21' : {'Name' : 'p6-t6', 'Vars' : ['Baseline', 'Angio36', 'RRC', 'TVC']}\n",
    "                   }\n",
    "animals_labels = ['p2-t2', 'p6-t6']\n",
    "\n",
    "def load_data_all_subjects(subjs_info_loading, subjs_info_final, fs, type_data = 'Field_Data_Neuro'):\n",
    "    subjects_names = list(subjs_info_loading.keys())\n",
    "    subjs_info = subjs_info_final\n",
    "    all_data = {}\n",
    "    print('Start loading ... \\n')\n",
    "    for subject in subjects_names:\n",
    "        name_subj, data_struct = load_data_one_subject(subject, subjs_info_loading, fs, type_data = type_data)\n",
    "        all_data[name_subj] = data_struct\n",
    "        print('================== %s loaded. =================='%subject)\n",
    "    return all_data\n",
    "\n",
    "\n",
    "def load_data_one_subject(subject, subjs_info_loading, fs, type_data = 'Field_Data_Neuro'):\n",
    "    name_subj_to_stock = subjs_info_loading[subject]['Name']\n",
    "    vars_to_load = subjs_info_loading[subject]['Vars']\n",
    "    data_struct = {}\n",
    "    \n",
    "    # file = h5py.File('data\\\\' + subject + '.mat','r')\n",
    "    file = h5py.File(subject + '.mat','r')\n",
    "    \n",
    "    # To get the names of the fields after decoding ASCII\n",
    "    all_field_names = get_field_names(file)\n",
    "\n",
    "    for var in vars_to_load:\n",
    "        id_field = np.where(all_field_names == var)[0]\n",
    "        curr_reference_data1 = file['Vagus_Data_Stimuli'][type_data][id_field][0][0]\n",
    "        curr_reference_data2 = file[curr_reference_data1][0][0]\n",
    "        final_data = np.transpose(np.asarray(file[curr_reference_data2]))\n",
    "        \n",
    "        if var == 'TV800' or var == 'TV500' or var == 'TV125': \n",
    "            var_name = 'TVC'\n",
    "        elif var == 'RR15' or var == 'RR20': \n",
    "            var_name = 'RRC'\n",
    "        else:\n",
    "            var_name = var\n",
    "            \n",
    "        data_struct[var_name] = {}\n",
    "        data_struct[var_name]['Data'] = final_data\n",
    "        n_time_pts = np.shape(final_data)[-1]\n",
    "        data_struct[var_name]['Time_pts'] = np.linspace(0, n_time_pts/fs, n_time_pts)\n",
    "        \n",
    "    return name_subj_to_stock, data_struct\n",
    "\n",
    "def get_field_names(file):\n",
    "    n_fields,_ = np.shape(file['Vagus_Data_Stimuli']['stimuli_name'])\n",
    "    all_field_names = []\n",
    "    for field in range(n_fields):\n",
    "        curr_reference_field = file['Vagus_Data_Stimuli']['stimuli_name'][field][0]\n",
    "        curr_field_ASCII = file[curr_reference_field]\n",
    "        curr_field = decode_ASCII(curr_field_ASCII)\n",
    "        all_field_names.append(curr_field)\n",
    "    return np.asarray(all_field_names)\n",
    "\n",
    "\n",
    "def decode_ASCII(numbers_array):\n",
    "    name = ''\n",
    "    squeezed_numbers = np.squeeze(numbers_array)\n",
    "    for n in squeezed_numbers:\n",
    "        name += chr(n)\n",
    "    return name\n",
    "\n",
    "data = load_data_all_subjects(subjs_info_loading, subjs_info_final, fs)\n",
    "print('Loading completed \\n')\n",
    "\n",
    "#print('Showing the data file:')\n",
    "#for key1 in data.keys():\n",
    "#    print('=========== %s ==========='%key1)\n",
    "#    for key2 in data[key1].keys():\n",
    "#        print('--- %s'%key2)\n",
    "#        for key3 in data[key1][key2].keys():\n",
    "#            print(' - %s'%key3)\n",
    "#            print('Shape : ', np.shape(data[key1][key2][key3]))\n",
    "\n",
    "def cut_all_data_to_established_duration_per_challenge(data):\n",
    "    '''\n",
    "    This function is a wrap-up to the function 'cut_data_one_pig_to_established_duration_per_challenge'.\n",
    "    '''\n",
    "    \n",
    "    new_data_struct = {}\n",
    "    for pig in data.keys():\n",
    "        print('================== Working on Pig %s =================='%pig)\n",
    "        data_curr_pig = data[pig]\n",
    "        data_curr_pig_cut = cut_data_one_pig_to_established_duration_per_challenge(data_curr_pig)\n",
    "        new_data_struct[pig] = data_curr_pig_cut\n",
    "        \n",
    "    return new_data_struct\n",
    "\n",
    "\n",
    "def cut_data_one_pig_to_established_duration_per_challenge(data_one_pig):\n",
    "    ''' \n",
    "    This function is used to cut the data for each challenge to the duration shown in Suppl. Table 1 in Vallone et al., 2021. \n",
    "    We take the first part of the data for each challenge (arbitrary choice). \n",
    "    '''\n",
    "    new_struct = copy.deepcopy(data_one_pig)\n",
    "    \n",
    "    dur_baseline = 5 #min\n",
    "    dur_RRC = 2 #min\n",
    "    dur_TVC = 2 #min\n",
    "    \n",
    "    for challenge in new_struct.keys():\n",
    "        data_curr_chal = new_struct[challenge]['Data']\n",
    "        time_pts_curr_chal = new_struct[challenge]['Time_pts']\n",
    "        t_end_sec = time_pts_curr_chal[-1]\n",
    "        \n",
    "        if challenge == 'Baseline': t_end_sec = dur_baseline * 60\n",
    "        elif challenge == 'RRC': t_end_sec = dur_RRC * 60\n",
    "        elif challenge == 'TVC': t_end_sec = dur_TVC * 60\n",
    "            \n",
    "#         print('Challenge %s , t_end_sec %0.3f'%(challenge, t_end_sec))\n",
    "            \n",
    "        id_t_end_curr_chal = find_specific_time_index(time_pts_curr_chal, t_end_sec)\n",
    "        new_struct[challenge]['Data'] = data_curr_chal[:,:id_t_end_curr_chal]\n",
    "        new_struct[challenge]['Time_pts'] = time_pts_curr_chal[:id_t_end_curr_chal]\n",
    "    \n",
    "    return new_struct\n",
    "\n",
    "def find_specific_time_index(time_pts, t):\n",
    "    t_id = np.argmin(np.abs(time_pts - t))\n",
    "    return t_id\n",
    "\n",
    "print('Start cutting data ... \\n')\n",
    "cut_data = cut_all_data_to_established_duration_per_challenge(data)\n",
    "print('Data cutting completed \\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data is structured pig -> challenge -> data, time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Showing the data files:\n",
      "=========== p2-t2 ===========\n",
      "--- Baseline\n",
      " - Data\n",
      "Shape :  (8, 7319999) , type:  <class 'numpy.ndarray'>\n",
      " - Time_pts\n",
      "Shape :  (7319999,) , type:  <class 'numpy.ndarray'>\n",
      "--- Angio36\n",
      " - Data\n",
      "Shape :  (8, 14780415) , type:  <class 'numpy.ndarray'>\n",
      " - Time_pts\n",
      "Shape :  (14780415,) , type:  <class 'numpy.ndarray'>\n",
      "--- RRC\n",
      " - Data\n",
      "Shape :  (8, 2927999) , type:  <class 'numpy.ndarray'>\n",
      " - Time_pts\n",
      "Shape :  (2927999,) , type:  <class 'numpy.ndarray'>\n",
      "--- TVC\n",
      " - Data\n",
      "Shape :  (8, 2927999) , type:  <class 'numpy.ndarray'>\n",
      " - Time_pts\n",
      "Shape :  (2927999,) , type:  <class 'numpy.ndarray'>\n",
      "=========== p6-t6 ===========\n",
      "--- Baseline\n",
      " - Data\n",
      "Shape :  (16, 7319999) , type:  <class 'numpy.ndarray'>\n",
      " - Time_pts\n",
      "Shape :  (7319999,) , type:  <class 'numpy.ndarray'>\n",
      "--- Angio36\n",
      " - Data\n",
      "Shape :  (16, 14940159) , type:  <class 'numpy.ndarray'>\n",
      " - Time_pts\n",
      "Shape :  (14940159,) , type:  <class 'numpy.ndarray'>\n",
      "--- RRC\n",
      " - Data\n",
      "Shape :  (16, 2927999) , type:  <class 'numpy.ndarray'>\n",
      " - Time_pts\n",
      "Shape :  (2927999,) , type:  <class 'numpy.ndarray'>\n",
      "--- TVC\n",
      " - Data\n",
      "Shape :  (16, 2927999) , type:  <class 'numpy.ndarray'>\n",
      " - Time_pts\n",
      "Shape :  (2927999,) , type:  <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "print('Showing the data files:')\n",
    "for key1 in cut_data.keys():\n",
    "    print('=========== %s ==========='%key1)\n",
    "    for key2 in cut_data[key1].keys():\n",
    "        print('--- %s'%key2)\n",
    "        for key3 in cut_data[key1][key2].keys():\n",
    "            print(' - %s'%key3)\n",
    "            print('Shape : ', np.shape(cut_data[key1][key2][key3]), ', type: ', type(cut_data[key1][key2][key3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Preprocessing #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The paper says they use a 4-order Butterworth filter [1000, 6000] Hz. \n",
    "\n",
    "Note that we have fs= 24400Hz, hence by Shannon the recorded signal can have no components above 2*fs 48000. \n",
    "\n",
    "Why do we bandpass at 6k Hz then???\n",
    "\n",
    "We highpass at 1kHz to remove any forms of noise, notably due to the organism itself. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_low = 1000 # [Hz]\n",
    "f_high = 6000 # [Hz]\n",
    "N = 4 # order of the filter\n",
    "sos = signal.butter(N, [f_low, f_high], 'bandpass', analog=False, fs=fs, output='sos')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bandpass_data(data, sos):\n",
    "    print('Bandpassing the data... \\n')\n",
    "    new_data = copy.deepcopy(data)\n",
    "    for pig in data.keys():\n",
    "        print('=========== %s ==========='%pig)\n",
    "        for challenge in data[pig].keys():\n",
    "            new_data[pig][challenge] = signal.sosfilt(sos, cut_data[pig][challenge]['Data'])\n",
    "    return new_data        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that here we remove the 'time' field, which is not used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bandpassing the data... \n",
      "\n",
      "=========== p2-t2 ===========\n",
      "=========== p6-t6 ===========\n"
     ]
    }
   ],
   "source": [
    "# takes ~40sec to run\n",
    "data_filtered = bandpass_data(cut_data, sos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sub-sampling of the signal by a factor 2: \n",
    "\n",
    "(Why???)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "# takes ~20sec to run\n",
    "data_subsampled = copy.deepcopy(data_filtered)\n",
    "# data_subsampled = data_blank\n",
    "for pig in data.keys():\n",
    "    for challenge in data[pig].keys():\n",
    "        data_subsampled[pig][challenge] = data_subsampled[pig][challenge][:,0:-1:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Split into test/train, perform windowing, extract features #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following the paper, we use a 90-10 train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(data, tt_split):\n",
    "    data_train = {'Baseline': {}, 'Angio36' : {}, 'RRC': {}, 'TVC': {}}\n",
    "    data_test = dict.fromkeys(data.keys(), {})\n",
    "    for challenge in data.keys():\n",
    "        L = data[challenge].shape[1]\n",
    "        data_train[challenge]= data[challenge][:,0:int(tt_split*L)]\n",
    "        data_test[challenge]= data[challenge][:,int(tt_split*L):]\n",
    "    return data_train, data_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Could make a dict of dicts but for two pigs it's unecessary\n",
    "tt_split = 0.9\n",
    "pig1 = 'p2-t2'\n",
    "pig2 = 'p6-t6'\n",
    "pig1_train_data, pig1_test_data = split_data(data_subsampled[pig1], tt_split)\n",
    "pig2_train_data, pig2_test_data = split_data(data_subsampled[pig2], tt_split)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following this we have the following data structure: four separate sets, each set contains the four challenges, for each challenge the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute features, as given in the paper. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the structure used:\n",
    "\n",
    "- compute_features computes features on the time series for a single channel, for a single challenge\n",
    "- windowing_and_features generates windows and computes features on each window from the [channels x time] array for a given challenge\n",
    "- data_windowing_and_features creates data and label vectors from the data {challenges x [channels x time]}\n",
    "\n",
    "Other implementation detail: it's easier to perform windowing and features computation in the same function, because that way the dimension you gain from windiwing is lost in computing features, then you don't need to use 3-d arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_features(data):\n",
    "    # [TODO]\n",
    "    '''\n",
    "    :param data: vector [timesteps]\n",
    "    '''\n",
    "    mean = ...\n",
    "    variance = ...\n",
    "    skew = ...\n",
    "    kurtosis = ...\n",
    "    mav = ...\n",
    "    max = ...\n",
    "    amp = ...\n",
    "    wh = ...\n",
    "    pow = ...\n",
    "    return np.array([mean, variance, skew, kurtosis, mav, max, amp, wh, pow])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "def windowing_and_features(data, windowSize_n, windowOverlap_n):\n",
    "    '''\n",
    "    :param data: ndarray [channels x timesteps]\n",
    "    '''\n",
    "    windowStep_n = windowSize_n - windowOverlap_n\n",
    "    features = np.array([])\n",
    "    for channel in data:\n",
    "        num_windows = int((channel.shape[0]-windowSize_n)/windowStep_n)\n",
    "        windows = np.array([])\n",
    "        for i in range(num_windows+1):\n",
    "            windows = np.append(windows, data[i*windowStep_n:i*windowStep_n+windowSize_n])\n",
    "        features = compute_features(windows)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_windowing_and_features(data, windowSize_n, windowOverlap_n):\n",
    "    '''\n",
    "    :param data: dictionnary with four keys (challenges), each challenge contains an ndarray [channels x timesteps]\n",
    "    '''\n",
    "    features = np.empty(0)\n",
    "    labels = np.empty(0)\n",
    "    for challenge in data.keys():\n",
    "        print(challenge)\n",
    "        challenge_features = windowing_and_features(data[challenge], windowSize_n, windowOverlap_n)\n",
    "        label = list(data).index(challenge) # converts to an index\n",
    "        challenge_labels = label* np.ones(challenge_features.shape[0])\n",
    "        features = np.append(features, challenge_features)\n",
    "        labels = np.append(labels, challenge_labels)\n",
    "    return features, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "windowSize_t = 50 # [ms]\n",
    "windowSize_n = windowSize_t * fs #[timesteps]\n",
    "windowOverlap_train_t = 20 #[ms]\n",
    "windowOverlap_train_n = windowOverlap_train_t * fs\n",
    "windowOverlap_test_n = 0\n",
    "\n",
    "pig1_train_X, pig1_train_y = data_windowing_and_features(pig1_train_data, windowSize_n, windowOverlap_train_n)\n",
    "pig1_test_X, pig1_test_y = data_windowing_and_features(pig1_test_data, windowSize_n, windowOverlap_test_n)\n",
    "\n",
    "pig2_train_X, pig2_train_y = data_windowing_and_features(pig2_train_data, windowSize_n, windowOverlap_train_n)\n",
    "pig2_test_X, pig2_test_y = data_windowing_and_features(pig2_test_data, windowSize_n, windowOverlap_test_n)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "548cb84f3c3c3a027d09fd18aa2745d959a35cd8c151ed2c8b5f74a76b0084e1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
